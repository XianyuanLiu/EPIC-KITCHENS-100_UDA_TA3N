(128, 207)
exp_path: model/action-model/Testexp-SGD-share_params_Y-lr_3e-3-bS_128_207/epic-5seg-disDA_none-alpha_0-advDA_RevGrad-beta_0.75_0.75_0.5-useBN_none-addlossDA_attentive_entropy-gamma_0.003-ensDA_none-mu_0-useAttn_TransAttn-n_attn_1/
ALL
Baseline: video
Frame aggregation method: trn-m
target data usage: uSv
Apply the adversarial-based Domain Adaptation approach: RevGrad
preparing the model......
Multi-Scale Temporal Relation Network Module in use ['5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']
using SGD
checking the checkpoint......
loading data......
start training......
Train: [1][0/126], lr: 0.00300	Time 1.308 (1.308)	Data 0.797 (0.797)	Prec@1 0.781 (0.781)	Prec@1 0.781 (0.781)	Prec@1 0.000 (0.000)	Prec@5 2.344 (2.344)	Prec@5 0.781 (0.781)	Prec@5 0.000 (0.000)	Loss 7.2516 (7.2516)   loss_verb 4.5795   loss_noun 5.7057	beta 0.012, 0.012, 0.008  loss_a 2.0829	gamma 0.003000  loss_e_verb 7.7454 loss_e_noun 9.6570	
Train: [1][50/126], lr: 0.00300	Time 0.105 (0.130)	Data 0.015 (0.033)	Prec@1 56.250 (34.666)	Prec@1 19.531 (10.172)	Prec@1 12.500 (4.963)	Prec@5 82.031 (67.310)	Prec@5 35.156 (25.352)	Prec@5 32.031 (20.098)	Loss 4.9776 (5.6732)   loss_verb 2.6622   loss_noun 4.5420	beta 0.017, 0.017, 0.012  loss_a 2.0520	gamma 0.003000  loss_e_verb 4.7199 loss_e_noun 7.9886	
Train: [1][100/126], lr: 0.00300	Time 0.107 (0.119)	Data 0.018 (0.027)	Prec@1 57.031 (46.310)	Prec@1 26.562 (17.466)	Prec@1 20.312 (11.556)	Prec@5 86.719 (75.201)	Prec@5 50.781 (36.510)	Prec@5 46.094 (31.846)	Loss 4.4593 (5.1435)   loss_verb 2.1639   loss_noun 4.0238	beta 0.022, 0.022, 0.015  loss_a 2.0325	gamma 0.003000  loss_e_verb 4.0376 loss_e_noun 7.3455	
Train: [2][0/126], lr: 0.00300	Time 0.856 (0.856)	Data 0.813 (0.813)	Prec@1 60.156 (60.156)	Prec@1 39.062 (39.062)	Prec@1 28.125 (28.125)	Prec@5 88.281 (88.281)	Prec@5 60.938 (60.938)	Prec@5 55.469 (55.469)	Loss 4.0812 (4.0812)   loss_verb 1.4841   loss_noun 2.7028	beta 0.025, 0.025, 0.017  loss_a 1.9744	gamma 0.003000  loss_e_verb 2.9785 loss_e_noun 5.9078	
Train: [2][50/126], lr: 0.00300	Time 0.115 (0.127)	Data 0.022 (0.038)	Prec@1 65.625 (65.717)	Prec@1 38.281 (37.669)	Prec@1 26.562 (28.248)	Prec@5 88.281 (88.358)	Prec@5 59.375 (62.669)	Prec@5 53.906 (57.797)	Loss 4.1155 (4.0524)   loss_verb 1.3373   loss_noun 2.8051	beta 0.030, 0.030, 0.020  loss_a 1.9684	gamma 0.003000  loss_e_verb 2.7954 loss_e_noun 5.7232	
Train: [2][100/126], lr: 0.00300	Time 0.104 (0.117)	Data 0.015 (0.029)	Prec@1 66.406 (66.074)	Prec@1 43.750 (39.472)	Prec@1 32.812 (29.788)	Prec@5 92.969 (88.714)	Prec@5 67.969 (65.509)	Prec@5 64.844 (60.605)	Loss 3.7330 (3.9565)   loss_verb 1.2997   loss_noun 2.6744	beta 0.035, 0.035, 0.023  loss_a 1.9570	gamma 0.003000  loss_e_verb 2.7510 loss_e_noun 5.5699	
Train: [3][0/126], lr: 0.00300	Time 0.866 (0.866)	Data 0.816 (0.816)	Prec@1 64.844 (64.844)	Prec@1 47.656 (47.656)	Prec@1 31.250 (31.250)	Prec@5 89.844 (89.844)	Prec@5 68.750 (68.750)	Prec@5 64.062 (64.062)	Loss 3.7937 (3.7937)   loss_verb 1.2478   loss_noun 2.4119	beta 0.037, 0.037, 0.025  loss_a 1.9517	gamma 0.003000  loss_e_verb 2.8707 loss_e_noun 5.2300	
Train: [3][50/126], lr: 0.00300	Time 0.095 (0.128)	Data 0.015 (0.038)	Prec@1 75.000 (70.052)	Prec@1 55.469 (48.330)	Prec@1 43.750 (37.454)	Prec@5 90.625 (91.222)	Prec@5 78.906 (74.755)	Prec@5 71.094 (69.899)	Loss 3.4029 (3.5624)   loss_verb 1.1166   loss_noun 2.1752	beta 0.042, 0.042, 0.028  loss_a 1.9050	gamma 0.003000  loss_e_verb 2.5720 loss_e_noun 5.0547	
Train: [3][100/126], lr: 0.00300	Time 0.103 (0.118)	Data 0.015 (0.029)	Prec@1 77.344 (70.614)	Prec@1 51.562 (48.987)	Prec@1 45.312 (38.034)	Prec@5 92.969 (91.646)	Prec@5 75.000 (75.951)	Prec@5 72.656 (71.272)	Loss 3.2660 (3.5138)   loss_verb 1.0879   loss_noun 2.1138	beta 0.047, 0.047, 0.032  loss_a 1.9019	gamma 0.003000  loss_e_verb 2.4608 loss_e_noun 4.9193	
Train: [4][0/126], lr: 0.00300	Time 0.858 (0.858)	Data 0.824 (0.824)	Prec@1 75.781 (75.781)	Prec@1 58.594 (58.594)	Prec@1 46.094 (46.094)	Prec@5 93.750 (93.750)	Prec@5 80.469 (80.469)	Prec@5 75.781 (75.781)	Loss 3.2720 (3.2720)   loss_verb 0.8530   loss_noun 1.8359	beta 0.050, 0.050, 0.033  loss_a 1.9169	gamma 0.003000  loss_e_verb 2.4296 loss_e_noun 4.6599	
Train: [4][50/126], lr: 0.00300	Time 0.114 (0.129)	Data 0.017 (0.038)	Prec@1 65.625 (73.392)	Prec@1 54.688 (55.316)	Prec@1 39.062 (43.689)	Prec@5 92.188 (93.444)	Prec@5 83.594 (80.622)	Prec@5 80.469 (76.394)	Loss 3.4133 (3.3290)   loss_verb 0.9624   loss_noun 1.8144	beta 0.055, 0.055, 0.037  loss_a 1.9307	gamma 0.003000  loss_e_verb 2.1966 loss_e_noun 4.3992	
Train: [4][100/126], lr: 0.00300	Time 0.114 (0.119)	Data 0.024 (0.029)	Prec@1 78.906 (73.028)	Prec@1 57.031 (55.863)	Prec@1 44.531 (44.036)	Prec@5 94.531 (93.417)	Prec@5 85.938 (81.088)	Prec@5 81.250 (76.949)	Loss 3.1195 (3.3117)   loss_verb 0.9622   loss_noun 1.7949	beta 0.060, 0.060, 0.040  loss_a 1.9234	gamma 0.003000  loss_e_verb 2.1709 loss_e_noun 4.3533	
Train: [5][0/126], lr: 0.00300	Time 0.832 (0.832)	Data 0.797 (0.797)	Prec@1 78.125 (78.125)	Prec@1 62.500 (62.500)	Prec@1 53.906 (53.906)	Prec@5 96.094 (96.094)	Prec@5 82.812 (82.812)	Prec@5 79.688 (79.688)	Loss 3.0493 (3.0493)   loss_verb 0.7322   loss_noun 1.5840	beta 0.062, 0.062, 0.042  loss_a 1.8811	gamma 0.003000  loss_e_verb 2.3599 loss_e_noun 4.3395	
Train: [5][50/126], lr: 0.00300	Time 0.101 (0.128)	Data 0.014 (0.037)	Prec@1 74.219 (75.031)	Prec@1 56.250 (59.926)	Prec@1 43.750 (47.809)	Prec@5 90.625 (94.164)	Prec@5 80.469 (84.145)	Prec@5 74.219 (80.300)	Loss 3.2882 (3.1289)   loss_verb 0.8703   loss_noun 1.5923	beta 0.067, 0.067, 0.045  loss_a 1.8886	gamma 0.003000  loss_e_verb 1.9950 loss_e_noun 4.0301	
Train: [5][100/126], lr: 0.00300	Time 0.113 (0.118)	Data 0.025 (0.029)	Prec@1 75.000 (75.015)	Prec@1 58.594 (60.249)	Prec@1 46.875 (48.066)	Prec@5 93.750 (94.191)	Prec@5 85.938 (84.576)	Prec@5 82.812 (80.732)	Loss 3.1910 (3.1258)   loss_verb 0.8694   loss_noun 1.5932	beta 0.072, 0.072, 0.048  loss_a 1.8856	gamma 0.003000  loss_e_verb 1.9670 loss_e_noun 3.9604	
Train: [6][0/126], lr: 0.00300	Time 0.876 (0.876)	Data 0.833 (0.833)	Prec@1 82.812 (82.812)	Prec@1 70.312 (70.312)	Prec@1 57.812 (57.812)	Prec@5 96.094 (96.094)	Prec@5 89.844 (89.844)	Prec@5 86.719 (86.719)	Loss 2.7329 (2.7329)   loss_verb 0.5585   loss_noun 1.1608	beta 0.075, 0.075, 0.050  loss_a 1.8652	gamma 0.003000  loss_e_verb 1.6674 loss_e_noun 3.7389	
Train: [6][50/126], lr: 0.00300	Time 0.099 (0.130)	Data 0.015 (0.040)	Prec@1 77.344 (77.604)	Prec@1 59.375 (63.526)	Prec@1 47.656 (51.792)	Prec@5 95.312 (95.144)	Prec@5 86.719 (87.102)	Prec@5 84.375 (83.747)	Loss 3.1414 (2.9947)   loss_verb 0.7920   loss_noun 1.4037	beta 0.080, 0.080, 0.053  loss_a 1.8888	gamma 0.003000  loss_e_verb 1.7465 loss_e_noun 3.6040	
Train: [6][100/126], lr: 0.00300	Time 0.111 (0.119)	Data 0.025 (0.030)	Prec@1 74.219 (77.212)	Prec@1 66.406 (63.838)	Prec@1 53.906 (51.856)	Prec@5 96.875 (95.243)	Prec@5 87.500 (86.928)	Prec@5 86.719 (83.741)	Loss 3.0171 (2.9924)   loss_verb 0.7824   loss_noun 1.4059	beta 0.085, 0.085, 0.056  loss_a 1.8903	gamma 0.003000  loss_e_verb 1.7323 loss_e_noun 3.5839	
Train: [7][0/126], lr: 0.00300	Time 0.884 (0.884)	Data 0.839 (0.839)	Prec@1 81.250 (81.250)	Prec@1 67.188 (67.188)	Prec@1 59.375 (59.375)	Prec@5 95.312 (95.312)	Prec@5 88.281 (88.281)	Prec@5 84.375 (84.375)	Loss 2.8617 (2.8617)   loss_verb 0.6607   loss_noun 1.2684	beta 0.087, 0.087, 0.058  loss_a 1.8894	gamma 0.003000  loss_e_verb 1.6879 loss_e_noun 3.5035	
Train: [7][50/126], lr: 0.00300	Time 0.106 (0.129)	Data 0.015 (0.038)	Prec@1 85.156 (79.305)	Prec@1 69.531 (66.360)	Prec@1 63.281 (55.193)	Prec@5 96.875 (95.787)	Prec@5 89.062 (88.634)	Prec@5 85.938 (85.662)	Loss 2.7062 (2.8708)   loss_verb 0.7095   loss_noun 1.2829	beta 0.092, 0.092, 0.061  loss_a 1.8672	gamma 0.003000  loss_e_verb 1.6070 loss_e_noun 3.3059	
Train: [7][100/126], lr: 0.00300	Time 0.111 (0.119)	Data 0.020 (0.029)	Prec@1 82.812 (79.107)	Prec@1 65.625 (66.437)	Prec@1 57.812 (55.005)	Prec@5 98.438 (95.924)	Prec@5 92.188 (88.591)	Prec@5 90.625 (85.744)	Loss 2.7504 (2.8846)   loss_verb 0.7187   loss_noun 1.2952	beta 0.097, 0.097, 0.065  loss_a 1.8704	gamma 0.003000  loss_e_verb 1.5677 loss_e_noun 3.2627	
Train: [8][0/126], lr: 0.00300	Time 0.872 (0.872)	Data 0.829 (0.829)	Prec@1 86.719 (86.719)	Prec@1 72.656 (72.656)	Prec@1 62.500 (62.500)	Prec@5 97.656 (97.656)	Prec@5 87.500 (87.500)	Prec@5 85.156 (85.156)	Loss 2.6890 (2.6890)   loss_verb 0.5124   loss_noun 1.1238	beta 0.099, 0.099, 0.066  loss_a 1.8636	gamma 0.003000  loss_e_verb 1.4991 loss_e_noun 3.3585	
Train: [8][50/126], lr: 0.00300	Time 0.116 (0.130)	Data 0.024 (0.038)	Prec@1 78.906 (80.699)	Prec@1 65.625 (69.256)	Prec@1 54.688 (57.828)	Prec@5 98.438 (96.492)	Prec@5 91.406 (90.579)	Prec@5 89.844 (87.714)	Loss 2.9117 (2.8148)   loss_verb 0.6559   loss_noun 1.1687	beta 0.104, 0.104, 0.070  loss_a 1.8959	gamma 0.003000  loss_e_verb 1.4011 loss_e_noun 2.9817	
Train: [8][100/126], lr: 0.00300	Time 0.117 (0.119)	Data 0.019 (0.030)	Prec@1 82.812 (80.183)	Prec@1 67.188 (69.415)	Prec@1 57.812 (57.797)	Prec@5 95.312 (96.488)	Prec@5 94.531 (90.532)	Prec@5 91.406 (87.809)	Loss 2.7880 (2.8167)   loss_verb 0.6704   loss_noun 1.1739	beta 0.109, 0.109, 0.073  loss_a 1.8880	gamma 0.003000  loss_e_verb 1.4143 loss_e_noun 2.9897	
Train: [9][0/126], lr: 0.00300	Time 0.860 (0.860)	Data 0.817 (0.817)	Prec@1 84.375 (84.375)	Prec@1 74.219 (74.219)	Prec@1 64.844 (64.844)	Prec@5 96.875 (96.875)	Prec@5 95.312 (95.312)	Prec@5 92.969 (92.969)	Loss 2.5787 (2.5787)   loss_verb 0.5930   loss_noun 0.8969	beta 0.112, 0.112, 0.074  loss_a 1.8275	gamma 0.003000  loss_e_verb 1.4035 loss_e_noun 2.7413	
Train: [9][50/126], lr: 0.00300	Time 0.107 (0.129)	Data 0.020 (0.038)	Prec@1 82.031 (82.935)	Prec@1 71.875 (71.676)	Prec@1 61.719 (61.489)	Prec@5 94.531 (97.319)	Prec@5 89.844 (91.973)	Prec@5 85.156 (89.813)	Loss 2.8546 (2.6958)   loss_verb 0.5911   loss_noun 1.0495	beta 0.117, 0.117, 0.078  loss_a 1.8692	gamma 0.003000  loss_e_verb 1.3676 loss_e_noun 2.8060	
Train: [9][100/126], lr: 0.00300	Time 0.111 (0.119)	Data 0.024 (0.028)	Prec@1 81.250 (81.861)	Prec@1 65.625 (71.094)	Prec@1 57.812 (60.110)	Prec@5 93.750 (96.952)	Prec@5 92.188 (91.700)	Prec@5 87.500 (89.202)	Loss 2.8611 (2.7225)   loss_verb 0.6237   loss_noun 1.0785	beta 0.121, 0.121, 0.081  loss_a 1.8652	gamma 0.003000  loss_e_verb 1.3382 loss_e_noun 2.7988	
Train: [10][0/126], lr: 0.00030	Time 0.848 (0.848)	Data 0.809 (0.809)	Prec@1 87.500 (87.500)	Prec@1 74.219 (74.219)	Prec@1 66.406 (66.406)	Prec@5 97.656 (97.656)	Prec@5 94.531 (94.531)	Prec@5 92.188 (92.188)	Loss 2.5759 (2.5759)   loss_verb 0.4734   loss_noun 0.9426	beta 0.124, 0.124, 0.083  loss_a 1.8618	gamma 0.003000  loss_e_verb 1.2831 loss_e_noun 2.7761	
Train: [10][50/126], lr: 0.00030	Time 0.114 (0.129)	Data 0.017 (0.038)	Prec@1 89.062 (84.007)	Prec@1 73.438 (74.234)	Prec@1 65.625 (64.047)	Prec@5 99.219 (97.426)	Prec@5 93.750 (92.678)	Prec@5 92.969 (90.732)	Loss 2.4911 (2.6223)   loss_verb 0.5367   loss_noun 0.9759	beta 0.129, 0.129, 0.086  loss_a 1.8599	gamma 0.003000  loss_e_verb 1.3024 loss_e_noun 2.7736	
Train: [10][100/126], lr: 0.00030	Time 0.110 (0.119)	Data 0.015 (0.028)	Prec@1 85.938 (84.360)	Prec@1 76.562 (75.015)	Prec@1 64.062 (64.991)	Prec@5 100.000 (97.579)	Prec@5 95.312 (93.209)	Prec@5 95.312 (91.306)	Loss 2.5561 (2.5797)   loss_verb 0.5156   loss_noun 0.9292	beta 0.133, 0.133, 0.089  loss_a 1.8513	gamma 0.003000  loss_e_verb 1.2849 loss_e_noun 2.7306	
Train: [11][0/126], lr: 0.00030	Time 0.890 (0.890)	Data 0.850 (0.850)	Prec@1 84.375 (84.375)	Prec@1 70.312 (70.312)	Prec@1 60.156 (60.156)	Prec@5 96.875 (96.875)	Prec@5 94.531 (94.531)	Prec@5 91.406 (91.406)	Loss 2.6399 (2.6399)   loss_verb 0.5493   loss_noun 1.0170	beta 0.136, 0.136, 0.091  loss_a 1.8507	gamma 0.003000  loss_e_verb 1.3031 loss_e_noun 2.7483	
Train: [11][50/126], lr: 0.00030	Time 0.109 (0.133)	Data 0.020 (0.039)	Prec@1 90.625 (85.600)	Prec@1 82.031 (77.451)	Prec@1 74.219 (68.153)	Prec@5 99.219 (97.901)	Prec@5 98.438 (94.256)	Prec@5 97.656 (92.509)	Loss 2.3259 (2.4968)   loss_verb 0.4681   loss_noun 0.8237	beta 0.141, 0.141, 0.094  loss_a 1.8453	gamma 0.003000  loss_e_verb 1.1939 loss_e_noun 2.5247	
Train: [11][100/126], lr: 0.00030	Time 0.097 (0.121)	Data 0.013 (0.030)	Prec@1 86.719 (85.551)	Prec@1 71.875 (76.725)	Prec@1 64.062 (67.420)	Prec@5 99.219 (97.734)	Prec@5 95.312 (94.059)	Prec@5 94.531 (92.218)	Loss 2.4950 (2.5205)   loss_verb 0.4765   loss_noun 0.8508	beta 0.146, 0.146, 0.097  loss_a 1.8513	gamma 0.003000  loss_e_verb 1.1925 loss_e_noun 2.5053	
Train: [12][0/126], lr: 0.00030	Time 0.889 (0.889)	Data 0.844 (0.844)	Prec@1 88.281 (88.281)	Prec@1 75.000 (75.000)	Prec@1 69.531 (69.531)	Prec@5 100.000 (100.000)	Prec@5 93.750 (93.750)	Prec@5 93.750 (93.750)	Loss 2.4418 (2.4418)   loss_verb 0.3498   loss_noun 0.8933	beta 0.148, 0.148, 0.099  loss_a 1.8147	gamma 0.003000  loss_e_verb 1.1428 loss_e_noun 2.5580	
Train: [12][50/126], lr: 0.00030	Time 0.113 (0.129)	Data 0.025 (0.038)	Prec@1 82.812 (86.351)	Prec@1 81.250 (78.692)	Prec@1 67.969 (69.393)	Prec@5 99.219 (98.192)	Prec@5 96.875 (94.378)	Prec@5 96.094 (92.892)	Loss 2.4646 (2.4665)   loss_verb 0.4436   loss_noun 0.7952	beta 0.153, 0.153, 0.102  loss_a 1.8416	gamma 0.003000  loss_e_verb 1.1630 loss_e_noun 2.4421	
Train: [12][100/126], lr: 0.00030	Time 0.101 (0.119)	Data 0.015 (0.028)	Prec@1 85.938 (86.077)	Prec@1 78.906 (77.839)	Prec@1 67.969 (68.472)	Prec@5 100.000 (98.051)	Prec@5 92.188 (94.299)	Prec@5 92.188 (92.729)	Loss 2.4414 (2.4851)   loss_verb 0.4520   loss_noun 0.8116	beta 0.158, 0.158, 0.105  loss_a 1.8480	gamma 0.003000  loss_e_verb 1.1527 loss_e_noun 2.4251	
Train: [13][0/126], lr: 0.00030	Time 0.877 (0.877)	Data 0.835 (0.835)	Prec@1 87.500 (87.500)	Prec@1 82.812 (82.812)	Prec@1 73.438 (73.438)	Prec@5 99.219 (99.219)	Prec@5 96.875 (96.875)	Prec@5 96.094 (96.094)	Loss 2.3716 (2.3716)   loss_verb 0.3572   loss_noun 0.6417	beta 0.160, 0.160, 0.107  loss_a 1.8667	gamma 0.003000  loss_e_verb 1.1592 loss_e_noun 2.4769	
Train: [13][50/126], lr: 0.00030	Time 0.105 (0.129)	Data 0.019 (0.038)	Prec@1 87.500 (87.025)	Prec@1 75.000 (78.569)	Prec@1 67.969 (69.301)	Prec@5 97.656 (98.177)	Prec@5 93.750 (94.776)	Prec@5 91.406 (93.168)	Loss 2.5382 (2.4621)   loss_verb 0.4348   loss_noun 0.7837	beta 0.165, 0.165, 0.110  loss_a 1.8476	gamma 0.003000  loss_e_verb 1.1399 loss_e_noun 2.3932	
Train: [13][100/126], lr: 0.00030	Time 0.114 (0.125)	Data 0.015 (0.031)	Prec@1 88.281 (86.719)	Prec@1 79.688 (78.287)	Prec@1 70.312 (69.137)	Prec@5 99.219 (98.066)	Prec@5 94.531 (94.284)	Prec@5 94.531 (92.744)	Loss 2.4423 (2.4818)   loss_verb 0.4434   loss_noun 0.8057	beta 0.169, 0.169, 0.113  loss_a 1.8520	gamma 0.003000  loss_e_verb 1.1289 loss_e_noun 2.3916	
Train: [14][0/126], lr: 0.00030	Time 0.860 (0.860)	Data 0.821 (0.821)	Prec@1 86.719 (86.719)	Prec@1 85.156 (85.156)	Prec@1 74.219 (74.219)	Prec@5 100.000 (100.000)	Prec@5 95.312 (95.312)	Prec@5 95.312 (95.312)	Loss 2.2909 (2.2909)   loss_verb 0.3295   loss_noun 0.6155	beta 0.172, 0.172, 0.115  loss_a 1.8135	gamma 0.003000  loss_e_verb 1.1383 loss_e_noun 2.1492	
Train: [14][50/126], lr: 0.00030	Time 0.106 (0.129)	Data 0.014 (0.038)	Prec@1 88.281 (87.255)	Prec@1 81.250 (78.906)	Prec@1 72.656 (70.098)	Prec@5 98.438 (98.146)	Prec@5 97.656 (94.593)	Prec@5 96.094 (93.030)	Loss 2.4104 (2.4517)   loss_verb 0.4228   loss_noun 0.7775	beta 0.177, 0.177, 0.118  loss_a 1.8464	gamma 0.003000  loss_e_verb 1.1227 loss_e_noun 2.3440	
Train: [14][100/126], lr: 0.00030	Time 0.114 (0.119)	Data 0.022 (0.029)	Prec@1 85.156 (86.866)	Prec@1 80.469 (78.489)	Prec@1 71.094 (69.562)	Prec@5 98.438 (98.151)	Prec@5 98.438 (94.353)	Prec@5 96.875 (92.853)	Loss 2.3585 (2.4601)   loss_verb 0.4286   loss_noun 0.7867	beta 0.181, 0.181, 0.121  loss_a 1.8472	gamma 0.003000  loss_e_verb 1.1156 loss_e_noun 2.3501	
Train: [15][0/126], lr: 0.00030	Time 0.857 (0.857)	Data 0.811 (0.811)	Prec@1 86.719 (86.719)	Prec@1 79.688 (79.688)	Prec@1 72.656 (72.656)	Prec@5 96.875 (96.875)	Prec@5 93.750 (93.750)	Prec@5 91.406 (91.406)	Loss 2.5164 (2.5164)   loss_verb 0.4886   loss_noun 0.8480	beta 0.184, 0.184, 0.122  loss_a 1.8430	gamma 0.003000  loss_e_verb 1.1051 loss_e_noun 2.3007	
Train: [15][50/126], lr: 0.00030	Time 0.101 (0.131)	Data 0.013 (0.039)	Prec@1 89.844 (87.745)	Prec@1 82.812 (79.488)	Prec@1 76.562 (71.170)	Prec@5 97.656 (98.269)	Prec@5 97.656 (95.006)	Prec@5 95.312 (93.627)	Loss 2.3799 (2.4262)   loss_verb 0.4101   loss_noun 0.7425	beta 0.188, 0.188, 0.126  loss_a 1.8449	gamma 0.003000  loss_e_verb 1.0773 loss_e_noun 2.2824	
Train: [15][100/126], lr: 0.00030	Time 0.113 (0.120)	Data 0.025 (0.030)	Prec@1 90.625 (87.415)	Prec@1 81.250 (79.223)	Prec@1 76.562 (70.653)	Prec@5 99.219 (98.267)	Prec@5 97.656 (94.833)	Prec@5 96.875 (93.464)	Loss 2.2864 (2.4370)   loss_verb 0.4153   loss_noun 0.7524	beta 0.193, 0.193, 0.129  loss_a 1.8480	gamma 0.003000  loss_e_verb 1.0902 loss_e_noun 2.3012	
Train: [16][0/126], lr: 0.00030	Time 0.865 (0.865)	Data 0.833 (0.833)	Prec@1 89.844 (89.844)	Prec@1 82.031 (82.031)	Prec@1 73.438 (73.438)	Prec@5 98.438 (98.438)	Prec@5 96.094 (96.094)	Prec@5 94.531 (94.531)	Loss 2.3173 (2.3173)   loss_verb 0.2897   loss_noun 0.6233	beta 0.195, 0.195, 0.130  loss_a 1.8557	gamma 0.003000  loss_e_verb 1.0903 loss_e_noun 2.2841	
Train: [16][50/126], lr: 0.00030	Time 0.109 (0.129)	Data 0.014 (0.039)	Prec@1 82.812 (87.653)	Prec@1 72.656 (80.025)	Prec@1 65.625 (71.523)	Prec@5 95.312 (98.361)	Prec@5 90.625 (94.746)	Prec@5 89.062 (93.505)	Loss 2.6308 (2.4176)   loss_verb 0.3932   loss_noun 0.7357	beta 0.200, 0.200, 0.133  loss_a 1.8481	gamma 0.003000  loss_e_verb 1.0671 loss_e_noun 2.2749	
Train: [16][100/126], lr: 0.00030	Time 0.108 (0.119)	Data 0.020 (0.029)	Prec@1 89.844 (87.732)	Prec@1 84.375 (79.510)	Prec@1 77.344 (71.148)	Prec@5 98.438 (98.213)	Prec@5 96.094 (94.717)	Prec@5 95.312 (93.294)	Loss 2.2698 (2.4280)   loss_verb 0.4030   loss_noun 0.7481	beta 0.205, 0.205, 0.136  loss_a 1.8474	gamma 0.003000  loss_e_verb 1.0655 loss_e_noun 2.2790	
Train: [17][0/126], lr: 0.00030	Time 0.861 (0.861)	Data 0.827 (0.827)	Prec@1 90.625 (90.625)	Prec@1 82.812 (82.812)	Prec@1 75.000 (75.000)	Prec@5 100.000 (100.000)	Prec@5 96.094 (96.094)	Prec@5 96.094 (96.094)	Loss 2.2949 (2.2949)   loss_verb 0.2580   loss_noun 0.6825	beta 0.207, 0.207, 0.138  loss_a 1.8198	gamma 0.003000  loss_e_verb 1.0363 loss_e_noun 2.1984	
Train: [17][50/126], lr: 0.00030	Time 0.115 (0.130)	Data 0.020 (0.037)	Prec@1 89.062 (87.760)	Prec@1 79.688 (80.607)	Prec@1 71.094 (71.906)	Prec@5 96.094 (98.024)	Prec@5 91.406 (94.975)	Prec@5 89.844 (93.413)	Loss 2.4413 (2.4123)   loss_verb 0.4013   loss_noun 0.7211	beta 0.212, 0.212, 0.141  loss_a 1.8461	gamma 0.003000  loss_e_verb 1.0727 loss_e_noun 2.2569	
Train: [17][100/126], lr: 0.00030	Time 0.104 (0.119)	Data 0.015 (0.029)	Prec@1 86.719 (87.763)	Prec@1 78.906 (80.252)	Prec@1 69.531 (71.666)	Prec@5 99.219 (98.205)	Prec@5 96.875 (95.034)	Prec@5 96.094 (93.611)	Loss 2.4013 (2.4156)   loss_verb 0.4016   loss_noun 0.7281	beta 0.216, 0.216, 0.144  loss_a 1.8457	gamma 0.003000  loss_e_verb 1.0724 loss_e_noun 2.2586	
Train: [18][0/126], lr: 0.00030	Time 0.904 (0.904)	Data 0.867 (0.867)	Prec@1 83.594 (83.594)	Prec@1 79.688 (79.688)	Prec@1 67.969 (67.969)	Prec@5 98.438 (98.438)	Prec@5 97.656 (97.656)	Prec@5 96.875 (96.875)	Loss 2.4009 (2.4009)   loss_verb 0.4714   loss_noun 0.6043	beta 0.218, 0.218, 0.146  loss_a 1.8580	gamma 0.003000  loss_e_verb 1.1420 loss_e_noun 2.2806	
Train: [18][50/126], lr: 0.00030	Time 0.118 (0.130)	Data 0.025 (0.037)	Prec@1 84.375 (87.883)	Prec@1 76.562 (79.596)	Prec@1 64.844 (71.477)	Prec@5 99.219 (98.162)	Prec@5 93.750 (95.159)	Prec@5 92.969 (93.750)	Loss 2.4740 (2.4171)   loss_verb 0.4024   loss_noun 0.7273	beta 0.223, 0.223, 0.149  loss_a 1.8473	gamma 0.003000  loss_e_verb 1.0623 loss_e_noun 2.2306	
Train: [18][100/126], lr: 0.00030	Time 0.107 (0.119)	Data 0.018 (0.029)	Prec@1 89.062 (87.794)	Prec@1 82.812 (79.834)	Prec@1 75.000 (71.496)	Prec@5 96.875 (98.190)	Prec@5 97.656 (95.181)	Prec@5 94.531 (93.704)	Loss 2.3139 (2.4194)   loss_verb 0.4027   loss_noun 0.7300	beta 0.228, 0.228, 0.152  loss_a 1.8481	gamma 0.003000  loss_e_verb 1.0626 loss_e_noun 2.2386	
Train: [19][0/126], lr: 0.00030	Time 0.862 (0.862)	Data 0.826 (0.826)	Prec@1 86.719 (86.719)	Prec@1 79.688 (79.688)	Prec@1 68.750 (68.750)	Prec@5 100.000 (100.000)	Prec@5 95.312 (95.312)	Prec@5 95.312 (95.312)	Loss 2.4361 (2.4361)   loss_verb 0.3588   loss_noun 0.7211	beta 0.230, 0.230, 0.153  loss_a 1.8916	gamma 0.003000  loss_e_verb 0.9254 loss_e_noun 2.0824	
Train: [19][50/126], lr: 0.00030	Time 0.107 (0.129)	Data 0.020 (0.037)	Prec@1 88.281 (88.021)	Prec@1 81.250 (80.316)	Prec@1 72.656 (72.319)	Prec@5 100.000 (98.514)	Prec@5 94.531 (95.404)	Prec@5 94.531 (94.179)	Loss 2.3684 (2.4081)   loss_verb 0.3797   loss_noun 0.7144	beta 0.234, 0.234, 0.156  loss_a 1.8562	gamma 0.003000  loss_e_verb 1.0388 loss_e_noun 2.1859	
Train: [19][100/126], lr: 0.00030	Time 0.107 (0.119)	Data 0.023 (0.029)	Prec@1 85.938 (87.980)	Prec@1 86.719 (80.546)	Prec@1 77.344 (72.532)	Prec@5 99.219 (98.298)	Prec@5 97.656 (95.459)	Prec@5 96.875 (94.044)	Loss 2.4053 (2.4043)   loss_verb 0.3892   loss_noun 0.7106	beta 0.239, 0.239, 0.159  loss_a 1.8496	gamma 0.003000  loss_e_verb 1.0375 loss_e_noun 2.1972	
Train: [20][0/126], lr: 0.00003	Time 0.842 (0.842)	Data 0.803 (0.803)	Prec@1 86.719 (86.719)	Prec@1 80.469 (80.469)	Prec@1 69.531 (69.531)	Prec@5 99.219 (99.219)	Prec@5 93.750 (93.750)	Prec@5 92.969 (92.969)	Loss 2.3868 (2.3868)   loss_verb 0.3194   loss_noun 0.6994	beta 0.241, 0.241, 0.161  loss_a 1.8726	gamma 0.003000  loss_e_verb 1.0688 loss_e_noun 2.1215	
Train: [20][50/126], lr: 0.00003	Time 0.096 (0.129)	Data 0.014 (0.037)	Prec@1 88.281 (88.572)	Prec@1 82.031 (80.453)	Prec@1 75.000 (72.687)	Prec@5 97.656 (98.529)	Prec@5 96.875 (95.205)	Prec@5 95.312 (94.102)	Loss 2.4370 (2.3928)   loss_verb 0.3749   loss_noun 0.7053	beta 0.246, 0.246, 0.164  loss_a 1.8478	gamma 0.003000  loss_e_verb 1.0484 loss_e_noun 2.1759	
Train: [20][100/126], lr: 0.00003	Time 0.112 (0.119)	Data 0.022 (0.029)	Prec@1 83.594 (88.761)	Prec@1 79.688 (80.662)	Prec@1 67.969 (72.966)	Prec@5 97.656 (98.484)	Prec@5 94.531 (95.282)	Prec@5 92.188 (94.067)	Loss 2.3461 (2.3861)   loss_verb 0.3710   loss_noun 0.7000	beta 0.250, 0.250, 0.167  loss_a 1.8457	gamma 0.003000  loss_e_verb 1.0411 loss_e_noun 2.1787	
Train: [21][0/126], lr: 0.00003	Time 0.924 (0.924)	Data 0.881 (0.881)	Prec@1 83.594 (83.594)	Prec@1 77.344 (77.344)	Prec@1 64.062 (64.062)	Prec@5 99.219 (99.219)	Prec@5 96.094 (96.094)	Prec@5 95.312 (95.312)	Loss 2.4292 (2.4292)   loss_verb 0.4971   loss_noun 0.7430	beta 0.252, 0.252, 0.168  loss_a 1.8040	gamma 0.003000  loss_e_verb 1.0859 loss_e_noun 2.3077	
Train: [21][50/126], lr: 0.00003	Time 0.112 (0.130)	Data 0.018 (0.040)	Prec@1 83.594 (88.205)	Prec@1 80.469 (81.878)	Prec@1 70.312 (73.591)	Prec@5 97.656 (98.208)	Prec@5 96.094 (95.588)	Prec@5 94.531 (94.010)	Loss 2.4086 (2.3833)   loss_verb 0.3904   loss_noun 0.6740	beta 0.257, 0.257, 0.171  loss_a 1.8462	gamma 0.003000  loss_e_verb 1.0558 loss_e_noun 2.1913	
Train: [21][100/126], lr: 0.00003	Time 0.110 (0.119)	Data 0.024 (0.030)	Prec@1 84.375 (88.699)	Prec@1 78.906 (81.722)	Prec@1 69.531 (73.824)	Prec@5 96.875 (98.391)	Prec@5 95.312 (95.668)	Prec@5 92.969 (94.346)	Loss 2.5085 (2.3758)   loss_verb 0.3780   loss_noun 0.6749	beta 0.261, 0.261, 0.174  loss_a 1.8445	gamma 0.003000  loss_e_verb 1.0373 loss_e_noun 2.1755	
Train: [22][0/126], lr: 0.00003	Time 0.909 (0.909)	Data 0.856 (0.856)	Prec@1 89.844 (89.844)	Prec@1 85.156 (85.156)	Prec@1 77.344 (77.344)	Prec@5 96.875 (96.875)	Prec@5 95.312 (95.312)	Prec@5 92.188 (92.188)	Loss 2.3396 (2.3396)   loss_verb 0.3936   loss_noun 0.5831	beta 0.263, 0.263, 0.176  loss_a 1.8466	gamma 0.003000  loss_e_verb 1.0185 loss_e_noun 2.0966	
Train: [22][50/126], lr: 0.00003	Time 0.117 (0.129)	Data 0.025 (0.038)	Prec@1 92.188 (88.434)	Prec@1 82.031 (81.066)	Prec@1 75.781 (72.840)	Prec@5 99.219 (98.453)	Prec@5 97.656 (95.665)	Prec@5 97.656 (94.347)	Loss 2.2850 (2.3792)   loss_verb 0.3672   loss_noun 0.6849	beta 0.268, 0.268, 0.178  loss_a 1.8483	gamma 0.003000  loss_e_verb 1.0262 loss_e_noun 2.1667	
Train: [22][100/126], lr: 0.00003	Time 0.116 (0.119)	Data 0.030 (0.030)	Prec@1 89.062 (88.614)	Prec@1 85.156 (81.327)	Prec@1 76.562 (73.352)	Prec@5 98.438 (98.422)	Prec@5 96.875 (95.668)	Prec@5 96.094 (94.369)	Loss 2.3090 (2.3795)   loss_verb 0.3730   loss_noun 0.6793	beta 0.272, 0.272, 0.181  loss_a 1.8486	gamma 0.003000  loss_e_verb 1.0163 loss_e_noun 2.1587	
Train: [23][0/126], lr: 0.00003	Time 0.895 (0.895)	Data 0.843 (0.843)	Prec@1 94.531 (94.531)	Prec@1 78.125 (78.125)	Prec@1 75.000 (75.000)	Prec@5 99.219 (99.219)	Prec@5 94.531 (94.531)	Prec@5 94.531 (94.531)	Loss 2.2889 (2.2889)   loss_verb 0.2286   loss_noun 0.7584	beta 0.274, 0.274, 0.183  loss_a 1.7905	gamma 0.003000  loss_e_verb 1.0040 loss_e_noun 2.1994	
Train: [23][50/126], lr: 0.00003	Time 0.106 (0.132)	Data 0.013 (0.039)	Prec@1 92.188 (89.047)	Prec@1 84.375 (81.556)	Prec@1 78.125 (73.866)	Prec@5 99.219 (98.346)	Prec@5 98.438 (95.879)	Prec@5 97.656 (94.516)	Loss 2.2448 (2.3706)   loss_verb 0.3661   loss_noun 0.6722	beta 0.278, 0.278, 0.186  loss_a 1.8467	gamma 0.003000  loss_e_verb 1.0123 loss_e_noun 2.1593	
Train: [23][100/126], lr: 0.00003	Time 0.107 (0.121)	Data 0.018 (0.030)	Prec@1 88.281 (88.637)	Prec@1 80.469 (81.119)	Prec@1 71.875 (73.283)	Prec@5 97.656 (98.329)	Prec@5 96.094 (95.831)	Prec@5 93.750 (94.431)	Loss 2.3904 (2.3828)   loss_verb 0.3793   loss_noun 0.6787	beta 0.283, 0.283, 0.189  loss_a 1.8490	gamma 0.003000  loss_e_verb 1.0218 loss_e_noun 2.1698	
Train: [24][0/126], lr: 0.00003	Time 0.872 (0.872)	Data 0.835 (0.835)	Prec@1 89.844 (89.844)	Prec@1 85.938 (85.938)	Prec@1 78.906 (78.906)	Prec@5 99.219 (99.219)	Prec@5 96.875 (96.875)	Prec@5 96.094 (96.094)	Loss 2.2459 (2.2459)   loss_verb 0.3533   loss_noun 0.5325	beta 0.285, 0.285, 0.190  loss_a 1.7985	gamma 0.003000  loss_e_verb 0.9371 loss_e_noun 2.0846	
Train: [24][50/126], lr: 0.00003	Time 0.109 (0.130)	Data 0.022 (0.040)	Prec@1 87.500 (88.787)	Prec@1 82.031 (81.771)	Prec@1 71.875 (73.989)	Prec@5 98.438 (98.468)	Prec@5 96.094 (95.849)	Prec@5 94.531 (94.623)	Loss 2.3441 (2.3728)   loss_verb 0.3759   loss_noun 0.6686	beta 0.289, 0.289, 0.193  loss_a 1.8457	gamma 0.003000  loss_e_verb 1.0285 loss_e_noun 2.1719	
Train: [24][100/126], lr: 0.00003	Time 0.111 (0.120)	Data 0.024 (0.030)	Prec@1 89.062 (89.016)	Prec@1 80.469 (81.822)	Prec@1 74.219 (74.110)	Prec@5 100.000 (98.592)	Prec@5 97.656 (95.838)	Prec@5 97.656 (94.701)	Loss 2.3877 (2.3668)   loss_verb 0.3634   loss_noun 0.6672	beta 0.293, 0.293, 0.196  loss_a 1.8467	gamma 0.003000  loss_e_verb 1.0207 loss_e_noun 2.1616	
Train: [25][0/126], lr: 0.00003	Time 0.856 (0.856)	Data 0.827 (0.827)	Prec@1 90.625 (90.625)	Prec@1 78.125 (78.125)	Prec@1 71.094 (71.094)	Prec@5 98.438 (98.438)	Prec@5 92.188 (92.188)	Prec@5 90.625 (90.625)	Loss 2.4102 (2.4102)   loss_verb 0.3390   loss_noun 0.8103	beta 0.296, 0.296, 0.197  loss_a 1.8304	gamma 0.003000  loss_e_verb 1.0859 loss_e_noun 2.3546	
Train: [25][50/126], lr: 0.00003	Time 0.117 (0.129)	Data 0.023 (0.036)	Prec@1 91.406 (88.955)	Prec@1 84.375 (81.235)	Prec@1 78.125 (73.361)	Prec@5 99.219 (98.376)	Prec@5 98.438 (95.680)	Prec@5 97.656 (94.271)	Loss 2.2608 (2.3788)   loss_verb 0.3743   loss_noun 0.6719	beta 0.300, 0.300, 0.200  loss_a 1.8509	gamma 0.003000  loss_e_verb 1.0200 loss_e_noun 2.1441	
Train: [25][100/126], lr: 0.00003	Time 0.107 (0.119)	Data 0.018 (0.028)	Prec@1 89.844 (88.428)	Prec@1 83.594 (81.033)	Prec@1 77.344 (72.958)	Prec@5 99.219 (98.383)	Prec@5 96.094 (95.475)	Prec@5 95.312 (94.114)	Loss 2.2894 (2.3875)   loss_verb 0.3855   loss_noun 0.6846	beta 0.304, 0.304, 0.203  loss_a 1.8477	gamma 0.003000  loss_e_verb 1.0257 loss_e_noun 2.1534	
Train: [26][0/126], lr: 0.00003	Time 0.879 (0.879)	Data 0.844 (0.844)	Prec@1 90.625 (90.625)	Prec@1 83.594 (83.594)	Prec@1 75.781 (75.781)	Prec@5 99.219 (99.219)	Prec@5 97.656 (97.656)	Prec@5 96.875 (96.875)	Loss 2.3373 (2.3373)   loss_verb 0.3666   loss_noun 0.5496	beta 0.306, 0.306, 0.204  loss_a 1.8746	gamma 0.003000  loss_e_verb 0.9738 loss_e_noun 2.0688	
Train: [26][50/126], lr: 0.00003	Time 0.107 (0.129)	Data 0.025 (0.038)	Prec@1 88.281 (88.664)	Prec@1 78.125 (82.062)	Prec@1 68.750 (73.943)	Prec@5 98.438 (98.330)	Prec@5 96.875 (95.849)	Prec@5 95.312 (94.485)	Loss 2.3573 (2.3720)   loss_verb 0.3789   loss_noun 0.6541	beta 0.310, 0.310, 0.207  loss_a 1.8508	gamma 0.003000  loss_e_verb 1.0034 loss_e_noun 2.1340	
Train: [26][100/126], lr: 0.00003	Time 0.107 (0.119)	Data 0.015 (0.029)	Prec@1 89.844 (88.769)	Prec@1 79.688 (81.606)	Prec@1 71.875 (73.739)	Prec@5 96.875 (98.438)	Prec@5 93.750 (95.645)	Prec@5 91.406 (94.392)	Loss 2.4168 (2.3774)   loss_verb 0.3732   loss_noun 0.6729	beta 0.314, 0.314, 0.210  loss_a 1.8497	gamma 0.003000  loss_e_verb 1.0094 loss_e_noun 2.1394	
Train: [27][0/126], lr: 0.00003	Time 0.877 (0.877)	Data 0.838 (0.838)	Prec@1 90.625 (90.625)	Prec@1 75.781 (75.781)	Prec@1 68.750 (68.750)	Prec@5 99.219 (99.219)	Prec@5 97.656 (97.656)	Prec@5 96.875 (96.875)	Loss 2.4364 (2.4364)   loss_verb 0.3265   loss_noun 0.7393	beta 0.316, 0.316, 0.211  loss_a 1.8987	gamma 0.003000  loss_e_verb 1.0591 loss_e_noun 2.1397	
Train: [27][50/126], lr: 0.00003	Time 0.117 (0.131)	Data 0.029 (0.038)	Prec@1 91.406 (88.863)	Prec@1 82.031 (80.790)	Prec@1 76.562 (72.963)	Prec@5 99.219 (98.529)	Prec@5 98.438 (95.450)	Prec@5 97.656 (94.271)	Loss 2.2995 (2.3842)   loss_verb 0.3716   loss_noun 0.6848	beta 0.320, 0.320, 0.214  loss_a 1.8512	gamma 0.003000  loss_e_verb 0.9993 loss_e_noun 2.1291	
Train: [27][100/126], lr: 0.00003	Time 0.099 (0.120)	Data 0.020 (0.029)	Prec@1 82.031 (88.830)	Prec@1 78.125 (80.972)	Prec@1 67.188 (73.213)	Prec@5 98.438 (98.492)	Prec@5 96.094 (95.475)	Prec@5 94.531 (94.291)	Loss 2.4253 (2.3777)   loss_verb 0.3682   loss_noun 0.6821	beta 0.325, 0.325, 0.216  loss_a 1.8479	gamma 0.003000  loss_e_verb 1.0051 loss_e_noun 2.1435	
Train: [28][0/126], lr: 0.00003	Time 0.868 (0.868)	Data 0.825 (0.825)	Prec@1 85.938 (85.938)	Prec@1 87.500 (87.500)	Prec@1 78.125 (78.125)	Prec@5 99.219 (99.219)	Prec@5 96.875 (96.875)	Prec@5 96.875 (96.875)	Loss 2.2960 (2.2960)   loss_verb 0.4195   loss_noun 0.5028	beta 0.327, 0.327, 0.218  loss_a 1.8303	gamma 0.003000  loss_e_verb 1.0349 loss_e_noun 2.0334	
Train: [28][50/126], lr: 0.00003	Time 0.109 (0.130)	Data 0.023 (0.038)	Prec@1 87.500 (88.021)	Prec@1 83.594 (82.047)	Prec@1 73.438 (73.775)	Prec@5 98.438 (98.529)	Prec@5 94.531 (95.665)	Prec@5 92.969 (94.424)	Loss 2.3936 (2.3656)   loss_verb 0.3835   loss_noun 0.6505	beta 0.331, 0.331, 0.220  loss_a 1.8439	gamma 0.003000  loss_e_verb 1.0283 loss_e_noun 2.1518	
Train: [28][100/126], lr: 0.00003	Time 0.090 (0.119)	Data 0.015 (0.029)	Prec@1 93.750 (88.428)	Prec@1 78.125 (81.459)	Prec@1 73.438 (73.499)	Prec@5 100.000 (98.507)	Prec@5 96.875 (95.575)	Prec@5 96.875 (94.377)	Loss 2.4040 (2.3759)   loss_verb 0.3704   loss_noun 0.6725	beta 0.335, 0.335, 0.223  loss_a 1.8497	gamma 0.003000  loss_e_verb 1.0208 loss_e_noun 2.1473	
Train: [29][0/126], lr: 0.00003	Time 0.854 (0.854)	Data 0.816 (0.816)	Prec@1 83.594 (83.594)	Prec@1 75.781 (75.781)	Prec@1 65.625 (65.625)	Prec@5 97.656 (97.656)	Prec@5 89.844 (89.844)	Prec@5 89.062 (89.062)	Loss 2.5627 (2.5627)   loss_verb 0.4974   loss_noun 0.8675	beta 0.337, 0.337, 0.224  loss_a 1.8749	gamma 0.003000  loss_e_verb 1.2008 loss_e_noun 2.3287	
Train: [29][50/126], lr: 0.00003	Time 0.110 (0.129)	Data 0.014 (0.037)	Prec@1 85.156 (88.618)	Prec@1 82.812 (81.434)	Prec@1 72.656 (73.667)	Prec@5 96.875 (98.499)	Prec@5 94.531 (95.849)	Prec@5 91.406 (94.638)	Loss 2.3841 (2.3721)   loss_verb 0.3657   loss_noun 0.6724	beta 0.341, 0.341, 0.227  loss_a 1.8483	gamma 0.003000  loss_e_verb 1.0158 loss_e_noun 2.1587	
Train: [29][100/126], lr: 0.00003	Time 0.111 (0.119)	Data 0.020 (0.029)	Prec@1 89.062 (88.266)	Prec@1 80.469 (81.443)	Prec@1 72.656 (73.314)	Prec@5 100.000 (98.507)	Prec@5 96.094 (95.630)	Prec@5 96.094 (94.462)	Loss 2.3086 (2.3800)   loss_verb 0.3737   loss_noun 0.6786	beta 0.345, 0.345, 0.230  loss_a 1.8491	gamma 0.003000  loss_e_verb 1.0122 loss_e_noun 2.1422	
Train: [30][0/126], lr: 0.00003	Time 0.891 (0.891)	Data 0.853 (0.853)	Prec@1 88.281 (88.281)	Prec@1 84.375 (84.375)	Prec@1 75.781 (75.781)	Prec@5 99.219 (99.219)	Prec@5 98.438 (98.438)	Prec@5 97.656 (97.656)	Loss 2.2347 (2.2347)   loss_verb 0.3463   loss_noun 0.5507	beta 0.347, 0.347, 0.231  loss_a 1.7816	gamma 0.003000  loss_e_verb 0.9566 loss_e_noun 2.0589	
Train: [30][50/126], lr: 0.00003	Time 0.115 (0.129)	Data 0.025 (0.038)	Prec@1 87.500 (89.216)	Prec@1 83.594 (82.230)	Prec@1 74.219 (74.433)	Prec@5 98.438 (98.713)	Prec@5 95.312 (96.369)	Prec@5 94.531 (95.297)	Loss 2.3632 (2.3532)   loss_verb 0.3557   loss_noun 0.6415	beta 0.350, 0.350, 0.234  loss_a 1.8500	gamma 0.003000  loss_e_verb 1.0099 loss_e_noun 2.1278	
Train: [30][100/126], lr: 0.00003	Time 0.116 (0.119)	Data 0.025 (0.030)	Prec@1 86.719 (89.039)	Prec@1 80.469 (81.629)	Prec@1 71.875 (73.801)	Prec@5 97.656 (98.639)	Prec@5 95.312 (95.831)	Prec@5 92.969 (94.670)	Loss 2.4728 (2.3715)   loss_verb 0.3634   loss_noun 0.6657	beta 0.354, 0.354, 0.236  loss_a 1.8522	gamma 0.003000  loss_e_verb 1.0061 loss_e_noun 2.1370	
total training time: 431.2809908390045
checkpoint
testing on the test set
preparing the model......
Multi-Scale Temporal Relation Network Module in use ['5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']
loading data......
  0%|          | 0/16 [00:00<?, ?it/s]/home/ac1xliu/experiments/DAAR/epicuda/EPIC-KITCHENS-100_UDA_TA3N/models.py:477: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  base_out = (self.softmax(base_out[0]), self.softmax(base_out[1]))
Testing Results: Prec@1 verb 48.039  Prec@1 noun 27.865 Prec@1 action 19.264 Prec@5 verb 76.132 Prec@5 noun 49.937 Prec@5 action 43.031 Loss 4.81393
